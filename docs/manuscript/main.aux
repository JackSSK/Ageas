\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{naturemag-doi}
\citation{2020SciPy-NMeth}
\citation{transfac}
\citation{gkaa1057}
\citation{biogrid}
\citation{gkaa1057}
\citation{grnboost2}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\newlabel{introduction}{{}{1}{}{Doc-Start}{}}
\newlabel{method}{{}{1}{}{Doc-Start}{}}
\newlabel{step1}{{}{1}{}{figure.caption.1}{}}
\citation{grnboost2}
\citation{pcc_2012}
\citation{cid_2019}
\citation{grnboost2}
\citation{grnboost2}
\citation{hyperband}
\citation{hanley_mcneil_1982}
\citation{pytorch}
\citation{chen2016xgboost}
\citation{scikit-learn}
\citation{mostavi_chiu_huang_chen_2020}
\newlabel{step2}{{}{2}{}{figure.caption.2}{}}
\citation{transformer}
\citation{roth_1988}
\citation{lundberg2017unified}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Model selection algorithm\relax }}{3}{algocf.1}\protected@file@percent }
\newlabel{alg:one}{{1}{3}{}{algocf.1}{}}
\newlabel{step3}{{}{3}{}{figure.caption.5}{}}
\newlabel{features_importances}{{}{3}{}{figure.caption.5}{}}
\citation{lundberg2017unified}
\citation{chen2016xgboost}
\citation{scikit-learn}
\citation{chen2016xgboost}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces   Classification model algorithms integrated in \emph  {AGEAS} with correspond numbers of hyperparameter and preset model configurations. Categorical hyperparameters and continuous numerical hyperparameters are clarifed beside total number of hyperparameters ($\# \lambda $). Conditional hyperparameters which are required for selected other hyperparameters are shown in brackets if there is any. $\#$ Configs indicates total amount of configurations \emph  {AGEAS} applies by default. \relax }}{4}{table.caption.4}\protected@file@percent }
\newlabel{models}{{1}{4}{Classification model algorithms integrated in \emph {AGEAS} with correspond numbers of hyperparameter and preset model configurations. Categorical hyperparameters and continuous numerical hyperparameters are clarifed beside total number of hyperparameters ($\# \lambda $). Conditional hyperparameters which are required for selected other hyperparameters are shown in brackets if there is any. $\#$ Configs indicates total amount of configurations \emph {AGEAS} applies by default. \relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces   Classifier algorithms with applicable Shapley value approximating methods. Algorithms marked with \textbf  {*} have internalized feature importance estimating methods which will be applied with higher priority than Shapley value based methods. GBDT implemented with \emph  {XGBoost}\cite  {chen2016xgboost} can have feature importance approximated with average weight gain at each split involving the feature. Linear SVM implemented with \emph  {scikit-learn}\cite  {scikit-learn} can have the importance estimated with feature coefficient or using Linear Explainer. However, for SVM with kernel function, the feature coefficient estimation would be inappropriate, and feature importances should be approximated with Kernel Explainer. \relax }}{4}{table.caption.6}\protected@file@percent }
\newlabel{shap}{{2}{4}{Classifier algorithms with applicable Shapley value approximating methods. Algorithms marked with \textbf {*} have internalized feature importance estimating methods which will be applied with higher priority than Shapley value based methods. GBDT implemented with \emph {XGBoost}\cite {chen2016xgboost} can have feature importance approximated with average weight gain at each split involving the feature. Linear SVM implemented with \emph {scikit-learn}\cite {scikit-learn} can have the importance estimated with feature coefficient or using Linear Explainer. However, for SVM with kernel function, the feature coefficient estimation would be inappropriate, and feature importances should be approximated with Kernel Explainer. \relax }{table.caption.6}{}}
\newlabel{step4}{{}{4}{}{table.caption.6}{}}
\citation{yamanaka_2006}
\citation{cell_repro_review}
\citation{niwa_2007}
\citation{yamanaka_2006,ips7f,ipsOK,oct4_nanog_sox2_lin28,oct4_nanog_sox2}
\citation{ASCL1_dopaminergic_neuron_2021}
\citation{isl1_da}
\citation{phox2_caudal_da,phox2_rat_da}
\citation{ELF3_precursor_marker}
\citation{ebf1_striatum,ebf1_cell_diff}
\citation{ebf1_migration}
\citation{ASCL1_dopaminergic_neuron_2021}
\citation{sp1_hypertrophy}
\citation{Sp1_electrophysiologt,CM_mature}
\citation{esr1_cm,esr1_cm_growth}
\citation{cebpb_1,cebpb_2}
\newlabel{step5}{{}{5}{}{table.caption.6}{}}
\newlabel{res}{{}{5}{}{table.caption.6}{}}
\citation{CM_mature,cebpb_1}
\citation{cebpb_1,nkx25}
\citation{CM_mature,yap1}
\bibdata{reference}
\bibcite{2020SciPy-NMeth}{1}
\bibcite{transfac}{2}
\bibcite{gkaa1057}{3}
\bibcite{biogrid}{4}
\bibcite{grnboost2}{5}
\bibcite{pcc_2012}{6}
\bibcite{cid_2019}{7}
\bibcite{hyperband}{8}
\bibcite{hanley_mcneil_1982}{9}
\bibcite{pytorch}{10}
\bibcite{chen2016xgboost}{11}
\bibcite{scikit-learn}{12}
\bibcite{mostavi_chiu_huang_chen_2020}{13}
\bibcite{transformer}{14}
\bibcite{roth_1988}{15}
\bibcite{lundberg2017unified}{16}
\newlabel{disc}{{}{6}{}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\hspace  *{-\tocsep }References}{6}{figure.caption.7}\protected@file@percent }
\bibcite{yamanaka_2006}{17}
\bibcite{cell_repro_review}{18}
\bibcite{niwa_2007}{19}
\bibcite{ips7f}{20}
\bibcite{ipsOK}{21}
\bibcite{oct4_nanog_sox2_lin28}{22}
\bibcite{oct4_nanog_sox2}{23}
\bibcite{ASCL1_dopaminergic_neuron_2021}{24}
\bibcite{isl1_da}{25}
\bibcite{phox2_caudal_da}{26}
\bibcite{phox2_rat_da}{27}
\bibcite{ELF3_precursor_marker}{28}
\bibcite{ebf1_striatum}{29}
\bibcite{ebf1_cell_diff}{30}
\bibcite{ebf1_migration}{31}
\bibcite{sp1_hypertrophy}{32}
\bibcite{Sp1_electrophysiologt}{33}
\bibcite{CM_mature}{34}
\bibcite{esr1_cm}{35}
\bibcite{esr1_cm_growth}{36}
\bibcite{cebpb_1}{37}
\bibcite{cebpb_2}{38}
\bibcite{nkx25}{39}
\bibcite{yap1}{40}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces   GEO data source table. \relax }}{8}{table.caption.8}\protected@file@percent }
\newlabel{geo_table}{{3}{8}{GEO data source table. \relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The overall workflow of \emph  {AGEAS}: \textbf  {\emph  {(1)}} Reconstruct meta-level GRN (meta-GRN) with expression data of all samples. \textbf  {\emph  {(2)}} Build pseudo-samples with sliding window algorithm and reconstruct GRNs with GRPs identified in meta-GRN accordingly. \textbf  {\emph  {(3)}} Select best performing classifiers in predicting class labels of pseudo-sample GRNs (psGRNs). \textbf  {\emph  {(4)}} Interpret how top models make classifications and gradually exclude GRPs with low weights or outlier-level high weights. \textbf  {\emph  {(5)}} Repeatedly train classifiers with differnt set of psGRNs as training data to extract GRPs frequently ranked as top important features for decision. \textbf  {\emph  {(6)}} Reconstruct GRNs with extracted GRPs and GRPs excluded as significant outliers. \relax }}{9}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{workflow}{{1}{9}{The overall workflow of \emph {AGEAS}: \textbf {\emph {(1)}} Reconstruct meta-level GRN (meta-GRN) with expression data of all samples. \textbf {\emph {(2)}} Build pseudo-samples with sliding window algorithm and reconstruct GRNs with GRPs identified in meta-GRN accordingly. \textbf {\emph {(3)}} Select best performing classifiers in predicting class labels of pseudo-sample GRNs (psGRNs). \textbf {\emph {(4)}} Interpret how top models make classifications and gradually exclude GRPs with low weights or outlier-level high weights. \textbf {\emph {(5)}} Repeatedly train classifiers with differnt set of psGRNs as training data to extract GRPs frequently ranked as top important features for decision. \textbf {\emph {(6)}} Reconstruct GRNs with extracted GRPs and GRPs excluded as significant outliers. \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Workflow to reconstruct meta-GRN and psGRNs. \textbf  {\emph  {(1)}} Filter genes from GEMs with log2FC filter(optional), MWW filter, and $\sigma $ filter. \textbf  {\emph  {(2)}} Find candidate GRP gene pairs from either interaction database or predictions made by \emph  {GRNBoost2}\cite  {grnboost2}-like algorithm. \textbf  {\emph  {(3)}} Filter candidate GRP gene pairs with PCC filter and reconstruct meta-GRN with validated GRPs. \textbf  {\emph  {(4)}} Generate pseudo-samples with SWA. \textbf  {\emph  {(5)}} Utilize GRPs in meta-GRN as generic guidance to form candidate GRPs for pseudo-samples. \textbf  {\emph  {(6)}} Filter every candidate GRP for all pseudo-samples with PCC filter and reconstruct psGRNs with validated GRPs. \relax }}{10}{figure.caption.2}\protected@file@percent }
\newlabel{data_preprocess}{{2}{10}{Workflow to reconstruct meta-GRN and psGRNs. \textbf {\emph {(1)}} Filter genes from GEMs with log2FC filter(optional), MWW filter, and $\sigma $ filter. \textbf {\emph {(2)}} Find candidate GRP gene pairs from either interaction database or predictions made by \emph {GRNBoost2}\cite {grnboost2}-like algorithm. \textbf {\emph {(3)}} Filter candidate GRP gene pairs with PCC filter and reconstruct meta-GRN with validated GRPs. \textbf {\emph {(4)}} Generate pseudo-samples with SWA. \textbf {\emph {(5)}} Utilize GRPs in meta-GRN as generic guidance to form candidate GRPs for pseudo-samples. \textbf {\emph {(6)}} Filter every candidate GRP for all pseudo-samples with PCC filter and reconstruct psGRNs with validated GRPs. \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 1D-CNN with 2 convolution layer set.\relax }}{10}{figure.caption.5}\protected@file@percent }
\newlabel{1dCNN}{{3}{10}{1D-CNN with 2 convolution layer set.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  TFs forming largest regulons with GRPs extracted by \emph  {AGEAS}. Each TF has regulon size indicating amount of direct regulatory genes and $log_2$ gene expression levels in binary sample classes. \textbf  {\emph  {(a)}} Mouse embryonic fibroblast vs. Embryonic stem cell \textbf  {\emph  {(b)}} Purified dopaminergic neuron vs. Radial glial/neuronal co-culture \textbf  {\emph  {(c)}} 7 days postnatal cardiomyocyte vs. 28 days postnatal cardiomyocyte \textbf  {\emph  {(d)}} Hepatic stellate cell vs. Portal fibroblast (both after 6 weeks of CCl4 administration) \relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{result_figs}{{4}{11}{TFs forming largest regulons with GRPs extracted by \emph {AGEAS}. Each TF has regulon size indicating amount of direct regulatory genes and $log_2$ gene expression levels in binary sample classes. \textbf {\emph {(a)}} Mouse embryonic fibroblast vs. Embryonic stem cell \textbf {\emph {(b)}} Purified dopaminergic neuron vs. Radial glial/neuronal co-culture \textbf {\emph {(c)}} 7 days postnatal cardiomyocyte vs. 28 days postnatal cardiomyocyte \textbf {\emph {(d)}} Hepatic stellate cell vs. Portal fibroblast (both after 6 weeks of CCl4 administration) \relax }{figure.caption.7}{}}
\newlabel{LastPage}{{}{11}{}{page.11}{}}
\xdef\lastpage@lastpage{11}
\xdef\lastpage@lastpageHy{11}
\ttl@finishall
\gdef \@abspage@last{11}
